{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91244d17",
   "metadata": {},
   "source": [
    "# <font color = 'Yellow'> Tokenization in Natural Language Processing (NLP) </font>\n",
    "\n",
    "Tokenization in Natural Language Processing (NLP) is the process of breaking down text into smaller units called tokens. These tokens can be words, subwords, or even individual characters. It's a foundational step in NLP tasks, as it helps convert raw text into a format that can be processed by algorithms.\n",
    "\n",
    "There are different types of tokenization:\n",
    "1. **`Word Tokenization`** : Splitting text into individual words.\n",
    "2. **`Subword Tokenization`** : Breaking words into smaller meaningful units, often used in models like BERT.\n",
    "3. **`Character Tokenization`** : Treating each character as a token, useful for languages with complex scripts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff0fe7",
   "metadata": {},
   "source": [
    "## Import Required Libiraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5a7fc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\mohan govindasamy\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mohan govindasamy\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mohan govindasamy\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mohan govindasamy\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mohan govindasamy\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\mohan govindasamy\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mohan govindasamy\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mohan govindasamy\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mohan govindasamy\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mohan govindasamy\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mohan govindasamy\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mohan govindasamy\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mohan govindasamy\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mohan govindasamy\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 5.0/10.4 MB 30.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.2/10.4 MB 29.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.4/10.4 MB 24.9 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 23.1 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.30.2 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3\n"
     ]
    }
   ],
   "source": [
    "# Before start doing Tokenization, need to install using following commands, if you installed already, then ignore\n",
    "# ! pip uninstall nltk\n",
    "# ! pip install nltk\n",
    "# Above line will uninistal and install nltk\n",
    "\n",
    "! pip install transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52993eb",
   "metadata": {},
   "source": [
    "## <font color = 'Yellow'>Tokenization</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7274d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization in Natural Language Processing (NLP) is the process of breaking down text into smaller units called tokens.\n",
      "These tokens can be words, subwords, or even individual characters.\n",
      "It's a foundational step in NLP tasks, as it helps convert raw text into a format that can be processed by algorithms.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myword = '''Tokenization in Natural Language Processing (NLP) is the process of breaking down text into smaller units called tokens.\n",
    "These tokens can be words, subwords, or even individual characters.\n",
    "It's a foundational step in NLP tasks, as it helps convert raw text into a format that can be processed by algorithms.\n",
    "'''\n",
    "\n",
    "print(myword)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988563d4",
   "metadata": {},
   "source": [
    "### Sentence -> Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76d7ab19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens: ['Tokenization in Natural Language Processing (NLP) is the process of breaking down text into smaller units called tokens.', 'These tokens can be words, subwords, or even individual characters.', \"It's a foundational step in NLP tasks, as it helps convert raw text into a format that can be processed by algorithms.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "para = sent_tokenize(myword)\n",
    "print(\"Sentence Tokens:\", para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9aee367c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45c6b5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization in Natural Language Processing (NLP) is the process of breaking down text into smaller units called tokens.\n",
      "These tokens can be words, subwords, or even individual characters.\n",
      "It's a foundational step in NLP tasks, as it helps convert raw text into a format that can be processed by algorithms.\n"
     ]
    }
   ],
   "source": [
    "for sentence in para:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40dafbc",
   "metadata": {},
   "source": [
    "## <font color = 'yellow'> Word Tokenization (Using NLTK) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffe077f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['Tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'the', 'process', 'of', 'breaking', 'down', 'text', 'into', 'smaller', 'units', 'called', 'tokens', '.', 'These', 'tokens', 'can', 'be', 'words', ',', 'subwords', ',', 'or', 'even', 'individual', 'characters', '.', 'It', \"'s\", 'a', 'foundational', 'step', 'in', 'NLP', 'tasks', ',', 'as', 'it', 'helps', 'convert', 'raw', 'text', 'into', 'a', 'format', 'that', 'can', 'be', 'processed', 'by', 'algorithms', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Word Tokenization\n",
    "tokens = word_tokenize(myword)\n",
    "print(\"Word Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e89475",
   "metadata": {},
   "source": [
    "## <font color = 'yellow'> Subword Tokenization (Using Hugging Face Transformers) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3035f0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd77cd795c7340059b81939451efef38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mohan Govindasamy\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Mohan Govindasamy\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b4512077e54e07bf843c2db20f438c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0204a7810dd440b92019728b432466e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9400df9fc9b04b2f944de726a8e9da05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subword Tokens: ['token', '##ization', 'in', 'natural', 'language', 'processing', '(', 'nl', '##p', ')', 'is', 'the', 'process', 'of', 'breaking', 'down', 'text', 'into', 'smaller', 'units', 'called', 'token', '##s', '.', 'these', 'token', '##s', 'can', 'be', 'words', ',', 'sub', '##words', ',', 'or', 'even', 'individual', 'characters', '.', 'it', \"'\", 's', 'a', 'foundation', '##al', 'step', 'in', 'nl', '##p', 'tasks', ',', 'as', 'it', 'helps', 'convert', 'raw', 'text', 'into', 'a', 'format', 'that', 'can', 'be', 'processed', 'by', 'algorithms', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize into subwords\n",
    "subword_tokens = tokenizer.tokenize(myword)\n",
    "print(\"Subword Tokens:\", subword_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3af5db",
   "metadata": {},
   "source": [
    "## <font color = 'yellow'> Character Tokenization <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdbde47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Tokens: ['T', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'i', 'n', ' ', 'N', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'L', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'P', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', '(', 'N', 'L', 'P', ')', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', ' ', 'o', 'f', ' ', 'b', 'r', 'e', 'a', 'k', 'i', 'n', 'g', ' ', 'd', 'o', 'w', 'n', ' ', 't', 'e', 'x', 't', ' ', 'i', 'n', 't', 'o', ' ', 's', 'm', 'a', 'l', 'l', 'e', 'r', ' ', 'u', 'n', 'i', 't', 's', ' ', 'c', 'a', 'l', 'l', 'e', 'd', ' ', 't', 'o', 'k', 'e', 'n', 's', '.', '\\n', 'T', 'h', 'e', 's', 'e', ' ', 't', 'o', 'k', 'e', 'n', 's', ' ', 'c', 'a', 'n', ' ', 'b', 'e', ' ', 'w', 'o', 'r', 'd', 's', ',', ' ', 's', 'u', 'b', 'w', 'o', 'r', 'd', 's', ',', ' ', 'o', 'r', ' ', 'e', 'v', 'e', 'n', ' ', 'i', 'n', 'd', 'i', 'v', 'i', 'd', 'u', 'a', 'l', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', 's', '.', '\\n', 'I', 't', \"'\", 's', ' ', 'a', ' ', 'f', 'o', 'u', 'n', 'd', 'a', 't', 'i', 'o', 'n', 'a', 'l', ' ', 's', 't', 'e', 'p', ' ', 'i', 'n', ' ', 'N', 'L', 'P', ' ', 't', 'a', 's', 'k', 's', ',', ' ', 'a', 's', ' ', 'i', 't', ' ', 'h', 'e', 'l', 'p', 's', ' ', 'c', 'o', 'n', 'v', 'e', 'r', 't', ' ', 'r', 'a', 'w', ' ', 't', 'e', 'x', 't', ' ', 'i', 'n', 't', 'o', ' ', 'a', ' ', 'f', 'o', 'r', 'm', 'a', 't', ' ', 't', 'h', 'a', 't', ' ', 'c', 'a', 'n', ' ', 'b', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'e', 'd', ' ', 'b', 'y', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize into characters\n",
    "character_tokens = list(myword)\n",
    "print(\"Character Tokens:\", character_tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
