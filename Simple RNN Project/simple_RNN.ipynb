{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7444e47d",
   "metadata": {},
   "source": [
    "# Simple RNN Project - using Tesnsorflow inbuild IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34de68fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.19.0-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (2.2.3)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.1-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting streamlit\n",
      "  Using cached streamlit-1.45.0-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from keras) (2.2.2)\n",
      "Collecting rich (from keras)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: namex in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from keras) (0.0.9)\n",
      "Requirement already satisfied: h5py in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from keras) (3.13.0)\n",
      "Requirement already satisfied: optree in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from keras) (0.15.0)\n",
      "Collecting ml-dtypes (from keras)\n",
      "  Using cached ml_dtypes-0.5.1-cp311-cp311-win_amd64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from keras) (24.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from tensorflow) (5.29.4)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from tensorflow) (78.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.71.0-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Using cached markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from tensorboard) (0.7.2)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.8-cp311-cp311-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.5.0 (from streamlit)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from streamlit) (5.5.2)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from streamlit) (20.0.0)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from streamlit) (9.1.2)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Using cached GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from altair<6,>=4.0->streamlit) (1.37.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard)\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from rich->keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\mohan govindasamy\\documents\\ai & ml course training\\github_learnings\\generative_ai_learning\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Using cached keras-3.9.2-py3-none-any.whl (1.3 MB)\n",
      "Using cached tensorflow-2.19.0-cp311-cp311-win_amd64.whl (375.9 MB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached grpcio-1.71.0-cp311-cp311-win_amd64.whl (4.3 MB)\n",
      "Using cached ml_dtypes-0.5.1-cp311-cp311-win_amd64.whl (209 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached scikit_learn-1.6.1-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "Using cached matplotlib-3.10.1-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "Using cached streamlit-1.45.0-py3-none-any.whl (9.9 MB)\n",
      "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Using cached kiwisolver-1.4.8-cp311-cp311-win_amd64.whl (71 kB)\n",
      "Using cached markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Installing collected packages: toml, scikit-learn, requests, pydeck, ml-dtypes, MarkupSafe, markdown, kiwisolver, grpcio, click, blinker, werkzeug, rich, matplotlib, jsonschema-specifications, gitpython, tensorboard, keras, jsonschema, tensorflow, altair, streamlit\n",
      "\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   - --------------------------------------  1/22 [scikit-learn]\n",
      "   ----- ----------------------------------  3/22 [pydeck]\n",
      "   ------- --------------------------------  4/22 [ml-dtypes]\n",
      "   ---------- -----------------------------  6/22 [markdown]\n",
      "   -------------- -------------------------  8/22 [grpcio]\n",
      "   -------------- -------------------------  8/22 [grpcio]\n",
      "   ---------------- -----------------------  9/22 [click]\n",
      "   -------------------- ------------------- 11/22 [werkzeug]\n",
      "   -------------------- ------------------- 11/22 [werkzeug]\n",
      "   --------------------- ------------------ 12/22 [rich]\n",
      "   --------------------- ------------------ 12/22 [rich]\n",
      "   --------------------- ------------------ 12/22 [rich]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   ----------------------- ---------------- 13/22 [matplotlib]\n",
      "   --------------------------- ------------ 15/22 [gitpython]\n",
      "   --------------------------- ------------ 15/22 [gitpython]\n",
      "   ----------------------------- ---------- 16/22 [tensorboard]\n",
      "   ----------------------------- ---------- 16/22 [tensorboard]\n",
      "   ----------------------------- ---------- 16/22 [tensorboard]\n",
      "   ----------------------------- ---------- 16/22 [tensorboard]\n",
      "   ----------------------------- ---------- 16/22 [tensorboard]\n",
      "   ----------------------------- ---------- 16/22 [tensorboard]\n",
      "   ----------------------------- ---------- 16/22 [tensorboard]\n",
      "   ----------------------------- ---------- 16/22 [tensorboard]\n",
      "   ----------------------------- ---------- 16/22 [tensorboard]\n",
      "   ----------------------------- ---------- 16/22 [tensorboard]\n",
      "   ----------------------------- ---------- 16/22 [tensorboard]\n",
      "   ----------------------------- ---------- 16/22 [tensorboard]\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'c:\\\\Users\\\\Mohan Govindasamy\\\\Documents\\\\AI & ML Course Training\\\\GitHub_Learnings\\\\Generative_AI_Learning\\\\venv\\\\Scripts\\\\tensorboard.exe' -> 'c:\\\\Users\\\\Mohan Govindasamy\\\\Documents\\\\AI & ML Course Training\\\\GitHub_Learnings\\\\Generative_AI_Learning\\\\venv\\\\Scripts\\\\tensorboard.exe.deleteme'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install the required libraries for the project:\n",
    "# - tensorflow: for building and training the artificial neural network (ANN)\n",
    "# - Keras: A high-level API built on top of TensorFlow for building and training neural networks.\n",
    "# - numpy: for numerical computations\n",
    "# - pandas: for data manipulation and analysis\n",
    "# - scikit-learn: for preprocessing and evaluation\n",
    "# - matplotlib: for data visualization\n",
    "# - tensorboard: for visualizing the training process\n",
    "# - streamlit: for creating an interactive web application\n",
    "# %pip install keras tensorflow numpy pandas scikit-learn matplotlib tensorboard streamlit\n",
    "# run above line if you are not yet installed the libiraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd22e1",
   "metadata": {},
   "source": [
    "## <font color = 'yellow'> Importing Required Libiraies </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55dcf598",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Importing the IMDB dataset from Keras, which is a part of TensorFlow\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m imdb\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Importing sequence preprocessing utilities for padding and truncating sequences\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sequence\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "# Importing pandas for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Importing numpy for numerical computations\n",
    "import numpy as np\n",
    "\n",
    "# Importing TensorFlow for building and training neural networks\n",
    "import tensorflow\n",
    "\n",
    "# Importing the IMDB dataset from Keras, which is a part of TensorFlow\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# Importing sequence preprocessing utilities for padding and truncating sequences\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Importing Sequential to define a linear stack of layers for the model\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Importing Embedding layer to convert words into dense vectors of fixed size\n",
    "# Importing SimpleRNN layer to build a simple recurrent neural network\n",
    "# Importing Dense layer to add fully connected layers to the model\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5944966f",
   "metadata": {},
   "source": [
    "## <font color = 'yellow'>Load Data set from Tensorflow > Keras </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00f48758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000,)\n",
      "y_train shape: (25000,)\n",
      "x_test shape: (25000,)\n",
      "y_test shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "# initializing the maximun number of words to be used in the model\n",
    "max_features = 10000 # vocabulary size\n",
    "\n",
    "# load the IMDB dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "# The IMDB dataset is a set of 50,000 reviews from IMDB, labeled by sentiment (positive/negative).\n",
    "\n",
    "# Print the shape of the training and testing data\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)  \n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59ee3b",
   "metadata": {},
   "source": [
    "### Extras to analysis dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc026fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0] # Print the first training sample\n",
    "# The output is a list of integers representing the words in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90f99496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0] # Print the label of the first training sample\n",
    "# The output is 1, indicating a positive review.\n",
    "int(y_train[0]) # Convert the label to an integer for better readability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e71d2279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample review: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "Sample label: 1\n"
     ]
    }
   ],
   "source": [
    "sample_review = x_train[0] # Get the first review from the training set\n",
    "sample_label = y_train[0] # Get the corresponding label\n",
    "print(\"Sample review:\", sample_review) # Print the sample review\n",
    "print(\"Sample label:\", sample_label) # Print the sample label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11106024",
   "metadata": {},
   "source": [
    "## <font color = 'yellow'> Pre-Processing </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cee2d04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : the\n",
      "2 : and\n",
      "3 : a\n",
      "4 : of\n",
      "5 : to\n",
      "6 : is\n",
      "7 : br\n",
      "8 : in\n",
      "9 : it\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{34701: 'fawn',\n",
       " 52006: 'tsukino',\n",
       " 52007: 'nunnery',\n",
       " 16816: 'sonja',\n",
       " 63951: 'vani',\n",
       " 1408: 'woods',\n",
       " 16115: 'spiders',\n",
       " 2345: 'hanging',\n",
       " 2289: 'woody',\n",
       " 52008: 'trawling',\n",
       " 52009: \"hold's\",\n",
       " 11307: 'comically',\n",
       " 40830: 'localized',\n",
       " 30568: 'disobeying',\n",
       " 52010: \"'royale\",\n",
       " 40831: \"harpo's\",\n",
       " 52011: 'canet',\n",
       " 19313: 'aileen',\n",
       " 52012: 'acurately',\n",
       " 52013: \"diplomat's\",\n",
       " 25242: 'rickman',\n",
       " 6746: 'arranged',\n",
       " 52014: 'rumbustious',\n",
       " 52015: 'familiarness',\n",
       " 52016: \"spider'\",\n",
       " 68804: 'hahahah',\n",
       " 52017: \"wood'\",\n",
       " 40833: 'transvestism',\n",
       " 34702: \"hangin'\",\n",
       " 2338: 'bringing',\n",
       " 40834: 'seamier',\n",
       " 34703: 'wooded',\n",
       " 52018: 'bravora',\n",
       " 16817: 'grueling',\n",
       " 1636: 'wooden',\n",
       " 16818: 'wednesday',\n",
       " 52019: \"'prix\",\n",
       " 34704: 'altagracia',\n",
       " 52020: 'circuitry',\n",
       " 11585: 'crotch',\n",
       " 57766: 'busybody',\n",
       " 52021: \"tart'n'tangy\",\n",
       " 14129: 'burgade',\n",
       " 52023: 'thrace',\n",
       " 11038: \"tom's\",\n",
       " 52025: 'snuggles',\n",
       " 29114: 'francesco',\n",
       " 52027: 'complainers',\n",
       " 52125: 'templarios',\n",
       " 40835: '272',\n",
       " 52028: '273',\n",
       " 52130: 'zaniacs',\n",
       " 34706: '275',\n",
       " 27631: 'consenting',\n",
       " 40836: 'snuggled',\n",
       " 15492: 'inanimate',\n",
       " 52030: 'uality',\n",
       " 11926: 'bronte',\n",
       " 4010: 'errors',\n",
       " 3230: 'dialogs',\n",
       " 52031: \"yomada's\",\n",
       " 34707: \"madman's\",\n",
       " 30585: 'dialoge',\n",
       " 52033: 'usenet',\n",
       " 40837: 'videodrome',\n",
       " 26338: \"kid'\",\n",
       " 52034: 'pawed',\n",
       " 30569: \"'girlfriend'\",\n",
       " 52035: \"'pleasure\",\n",
       " 52036: \"'reloaded'\",\n",
       " 40839: \"kazakos'\",\n",
       " 52037: 'rocque',\n",
       " 52038: 'mailings',\n",
       " 11927: 'brainwashed',\n",
       " 16819: 'mcanally',\n",
       " 52039: \"tom''\",\n",
       " 25243: 'kurupt',\n",
       " 21905: 'affiliated',\n",
       " 52040: 'babaganoosh',\n",
       " 40840: \"noe's\",\n",
       " 40841: 'quart',\n",
       " 359: 'kids',\n",
       " 5034: 'uplifting',\n",
       " 7093: 'controversy',\n",
       " 21906: 'kida',\n",
       " 23379: 'kidd',\n",
       " 52041: \"error'\",\n",
       " 52042: 'neurologist',\n",
       " 18510: 'spotty',\n",
       " 30570: 'cobblers',\n",
       " 9878: 'projection',\n",
       " 40842: 'fastforwarding',\n",
       " 52043: 'sters',\n",
       " 52044: \"eggar's\",\n",
       " 52045: 'etherything',\n",
       " 40843: 'gateshead',\n",
       " 34708: 'airball',\n",
       " 25244: 'unsinkable',\n",
       " 7180: 'stern',\n",
       " 52046: \"cervi's\",\n",
       " 40844: 'dnd',\n",
       " 11586: 'dna',\n",
       " 20598: 'insecurity',\n",
       " 52047: \"'reboot'\",\n",
       " 11037: 'trelkovsky',\n",
       " 52048: 'jaekel',\n",
       " 52049: 'sidebars',\n",
       " 52050: \"sforza's\",\n",
       " 17633: 'distortions',\n",
       " 52051: 'mutinies',\n",
       " 30602: 'sermons',\n",
       " 40846: '7ft',\n",
       " 52052: 'boobage',\n",
       " 52053: \"o'bannon's\",\n",
       " 23380: 'populations',\n",
       " 52054: 'chulak',\n",
       " 27633: 'mesmerize',\n",
       " 52055: 'quinnell',\n",
       " 10307: 'yahoo',\n",
       " 52057: 'meteorologist',\n",
       " 42577: 'beswick',\n",
       " 15493: 'boorman',\n",
       " 40847: 'voicework',\n",
       " 52058: \"ster'\",\n",
       " 22922: 'blustering',\n",
       " 52059: 'hj',\n",
       " 27634: 'intake',\n",
       " 5621: 'morally',\n",
       " 40849: 'jumbling',\n",
       " 52060: 'bowersock',\n",
       " 52061: \"'porky's'\",\n",
       " 16821: 'gershon',\n",
       " 40850: 'ludicrosity',\n",
       " 52062: 'coprophilia',\n",
       " 40851: 'expressively',\n",
       " 19500: \"india's\",\n",
       " 34710: \"post's\",\n",
       " 52063: 'wana',\n",
       " 5283: 'wang',\n",
       " 30571: 'wand',\n",
       " 25245: 'wane',\n",
       " 52321: 'edgeways',\n",
       " 34711: 'titanium',\n",
       " 40852: 'pinta',\n",
       " 178: 'want',\n",
       " 30572: 'pinto',\n",
       " 52065: 'whoopdedoodles',\n",
       " 21908: 'tchaikovsky',\n",
       " 2103: 'travel',\n",
       " 52066: \"'victory'\",\n",
       " 11928: 'copious',\n",
       " 22433: 'gouge',\n",
       " 52067: \"chapters'\",\n",
       " 6702: 'barbra',\n",
       " 30573: 'uselessness',\n",
       " 52068: \"wan'\",\n",
       " 27635: 'assimilated',\n",
       " 16116: 'petiot',\n",
       " 52069: 'most\\x85and',\n",
       " 3930: 'dinosaurs',\n",
       " 352: 'wrong',\n",
       " 52070: 'seda',\n",
       " 52071: 'stollen',\n",
       " 34712: 'sentencing',\n",
       " 40853: 'ouroboros',\n",
       " 40854: 'assimilates',\n",
       " 40855: 'colorfully',\n",
       " 27636: 'glenne',\n",
       " 52072: 'dongen',\n",
       " 4760: 'subplots',\n",
       " 52073: 'kiloton',\n",
       " 23381: 'chandon',\n",
       " 34713: \"effect'\",\n",
       " 27637: 'snugly',\n",
       " 40856: 'kuei',\n",
       " 9092: 'welcomed',\n",
       " 30071: 'dishonor',\n",
       " 52075: 'concurrence',\n",
       " 23382: 'stoicism',\n",
       " 14896: \"guys'\",\n",
       " 52077: \"beroemd'\",\n",
       " 6703: 'butcher',\n",
       " 40857: \"melfi's\",\n",
       " 30623: 'aargh',\n",
       " 20599: 'playhouse',\n",
       " 11308: 'wickedly',\n",
       " 1180: 'fit',\n",
       " 52078: 'labratory',\n",
       " 40859: 'lifeline',\n",
       " 1927: 'screaming',\n",
       " 4287: 'fix',\n",
       " 52079: 'cineliterate',\n",
       " 52080: 'fic',\n",
       " 52081: 'fia',\n",
       " 34714: 'fig',\n",
       " 52082: 'fmvs',\n",
       " 52083: 'fie',\n",
       " 52084: 'reentered',\n",
       " 30574: 'fin',\n",
       " 52085: 'doctresses',\n",
       " 52086: 'fil',\n",
       " 12606: 'zucker',\n",
       " 31931: 'ached',\n",
       " 52088: 'counsil',\n",
       " 52089: 'paterfamilias',\n",
       " 13885: 'songwriter',\n",
       " 34715: 'shivam',\n",
       " 9654: 'hurting',\n",
       " 299: 'effects',\n",
       " 52090: 'slauther',\n",
       " 52091: \"'flame'\",\n",
       " 52092: 'sommerset',\n",
       " 52093: 'interwhined',\n",
       " 27638: 'whacking',\n",
       " 52094: 'bartok',\n",
       " 8775: 'barton',\n",
       " 21909: 'frewer',\n",
       " 52095: \"fi'\",\n",
       " 6192: 'ingrid',\n",
       " 30575: 'stribor',\n",
       " 52096: 'approporiately',\n",
       " 52097: 'wobblyhand',\n",
       " 52098: 'tantalisingly',\n",
       " 52099: 'ankylosaurus',\n",
       " 17634: 'parasites',\n",
       " 52100: 'childen',\n",
       " 52101: \"jenkins'\",\n",
       " 52102: 'metafiction',\n",
       " 17635: 'golem',\n",
       " 40860: 'indiscretion',\n",
       " 23383: \"reeves'\",\n",
       " 57781: \"inamorata's\",\n",
       " 52104: 'brittannica',\n",
       " 7916: 'adapt',\n",
       " 30576: \"russo's\",\n",
       " 48246: 'guitarists',\n",
       " 10553: 'abbott',\n",
       " 40861: 'abbots',\n",
       " 17649: 'lanisha',\n",
       " 40863: 'magickal',\n",
       " 52105: 'mattter',\n",
       " 52106: \"'willy\",\n",
       " 34716: 'pumpkins',\n",
       " 52107: 'stuntpeople',\n",
       " 30577: 'estimate',\n",
       " 40864: 'ugghhh',\n",
       " 11309: 'gameplay',\n",
       " 52108: \"wern't\",\n",
       " 40865: \"n'sync\",\n",
       " 16117: 'sickeningly',\n",
       " 40866: 'chiara',\n",
       " 4011: 'disturbed',\n",
       " 40867: 'portmanteau',\n",
       " 52109: 'ineffectively',\n",
       " 82143: \"duchonvey's\",\n",
       " 37519: \"nasty'\",\n",
       " 1285: 'purpose',\n",
       " 52112: 'lazers',\n",
       " 28105: 'lightened',\n",
       " 52113: 'kaliganj',\n",
       " 52114: 'popularism',\n",
       " 18511: \"damme's\",\n",
       " 30578: 'stylistics',\n",
       " 52115: 'mindgaming',\n",
       " 46449: 'spoilerish',\n",
       " 52117: \"'corny'\",\n",
       " 34718: 'boerner',\n",
       " 6792: 'olds',\n",
       " 52118: 'bakelite',\n",
       " 27639: 'renovated',\n",
       " 27640: 'forrester',\n",
       " 52119: \"lumiere's\",\n",
       " 52024: 'gaskets',\n",
       " 884: 'needed',\n",
       " 34719: 'smight',\n",
       " 1297: 'master',\n",
       " 25905: \"edie's\",\n",
       " 40868: 'seeber',\n",
       " 52120: 'hiya',\n",
       " 52121: 'fuzziness',\n",
       " 14897: 'genesis',\n",
       " 12607: 'rewards',\n",
       " 30579: 'enthrall',\n",
       " 40869: \"'about\",\n",
       " 52122: \"recollection's\",\n",
       " 11039: 'mutilated',\n",
       " 52123: 'fatherlands',\n",
       " 52124: \"fischer's\",\n",
       " 5399: 'positively',\n",
       " 34705: '270',\n",
       " 34720: 'ahmed',\n",
       " 9836: 'zatoichi',\n",
       " 13886: 'bannister',\n",
       " 52127: 'anniversaries',\n",
       " 30580: \"helm's\",\n",
       " 52128: \"'work'\",\n",
       " 34721: 'exclaimed',\n",
       " 52129: \"'unfunny'\",\n",
       " 52029: '274',\n",
       " 544: 'feeling',\n",
       " 52131: \"wanda's\",\n",
       " 33266: 'dolan',\n",
       " 52133: '278',\n",
       " 52134: 'peacoat',\n",
       " 40870: 'brawny',\n",
       " 40871: 'mishra',\n",
       " 40872: 'worlders',\n",
       " 52135: 'protags',\n",
       " 52136: 'skullcap',\n",
       " 57596: 'dastagir',\n",
       " 5622: 'affairs',\n",
       " 7799: 'wholesome',\n",
       " 52137: 'hymen',\n",
       " 25246: 'paramedics',\n",
       " 52138: 'unpersons',\n",
       " 52139: 'heavyarms',\n",
       " 52140: 'affaire',\n",
       " 52141: 'coulisses',\n",
       " 40873: 'hymer',\n",
       " 52142: 'kremlin',\n",
       " 30581: 'shipments',\n",
       " 52143: 'pixilated',\n",
       " 30582: \"'00s\",\n",
       " 18512: 'diminishing',\n",
       " 1357: 'cinematic',\n",
       " 14898: 'resonates',\n",
       " 40874: 'simplify',\n",
       " 40875: \"nature'\",\n",
       " 40876: 'temptresses',\n",
       " 16822: 'reverence',\n",
       " 19502: 'resonated',\n",
       " 34722: 'dailey',\n",
       " 52144: '2\\x85',\n",
       " 27641: 'treize',\n",
       " 52145: 'majo',\n",
       " 21910: 'kiya',\n",
       " 52146: 'woolnough',\n",
       " 39797: 'thanatos',\n",
       " 35731: 'sandoval',\n",
       " 40879: 'dorama',\n",
       " 52147: \"o'shaughnessy\",\n",
       " 4988: 'tech',\n",
       " 32018: 'fugitives',\n",
       " 30583: 'teck',\n",
       " 76125: \"'e'\",\n",
       " 40881: 'doesnâ€™t',\n",
       " 52149: 'purged',\n",
       " 657: 'saying',\n",
       " 41095: \"martians'\",\n",
       " 23418: 'norliss',\n",
       " 27642: 'dickey',\n",
       " 52152: 'dicker',\n",
       " 52153: \"'sependipity\",\n",
       " 8422: 'padded',\n",
       " 57792: 'ordell',\n",
       " 40882: \"sturges'\",\n",
       " 52154: 'independentcritics',\n",
       " 5745: 'tempted',\n",
       " 34724: \"atkinson's\",\n",
       " 25247: 'hounded',\n",
       " 52155: 'apace',\n",
       " 15494: 'clicked',\n",
       " 30584: \"'humor'\",\n",
       " 17177: \"martino's\",\n",
       " 52156: \"'supporting\",\n",
       " 52032: 'warmongering',\n",
       " 34725: \"zemeckis's\",\n",
       " 21911: 'lube',\n",
       " 52157: 'shocky',\n",
       " 7476: 'plate',\n",
       " 40883: 'plata',\n",
       " 40884: 'sturgess',\n",
       " 40885: \"nerds'\",\n",
       " 20600: 'plato',\n",
       " 34726: 'plath',\n",
       " 40886: 'platt',\n",
       " 52159: 'mcnab',\n",
       " 27643: 'clumsiness',\n",
       " 3899: 'altogether',\n",
       " 42584: 'massacring',\n",
       " 52160: 'bicenntinial',\n",
       " 40887: 'skaal',\n",
       " 14360: 'droning',\n",
       " 8776: 'lds',\n",
       " 21912: 'jaguar',\n",
       " 34727: \"cale's\",\n",
       " 1777: 'nicely',\n",
       " 4588: 'mummy',\n",
       " 18513: \"lot's\",\n",
       " 10086: 'patch',\n",
       " 50202: 'kerkhof',\n",
       " 52161: \"leader's\",\n",
       " 27644: \"'movie\",\n",
       " 52162: 'uncomfirmed',\n",
       " 40888: 'heirloom',\n",
       " 47360: 'wrangle',\n",
       " 52163: 'emotion\\x85',\n",
       " 52164: \"'stargate'\",\n",
       " 40889: 'pinoy',\n",
       " 40890: 'conchatta',\n",
       " 41128: 'broeke',\n",
       " 40891: 'advisedly',\n",
       " 17636: \"barker's\",\n",
       " 52166: 'descours',\n",
       " 772: 'lots',\n",
       " 9259: 'lotr',\n",
       " 9879: 'irs',\n",
       " 52167: 'lott',\n",
       " 40892: 'xvi',\n",
       " 34728: 'irk',\n",
       " 52168: 'irl',\n",
       " 6887: 'ira',\n",
       " 21913: 'belzer',\n",
       " 52169: 'irc',\n",
       " 27645: 'ire',\n",
       " 40893: 'requisites',\n",
       " 7693: 'discipline',\n",
       " 52961: 'lyoko',\n",
       " 11310: 'extend',\n",
       " 873: 'nature',\n",
       " 52170: \"'dickie'\",\n",
       " 40894: 'optimist',\n",
       " 30586: 'lapping',\n",
       " 3900: 'superficial',\n",
       " 52171: 'vestment',\n",
       " 2823: 'extent',\n",
       " 52172: 'tendons',\n",
       " 52173: \"heller's\",\n",
       " 52174: 'quagmires',\n",
       " 52175: 'miyako',\n",
       " 20601: 'moocow',\n",
       " 52176: \"coles'\",\n",
       " 40895: 'lookit',\n",
       " 52177: 'ravenously',\n",
       " 40896: 'levitating',\n",
       " 52178: 'perfunctorily',\n",
       " 30587: 'lookin',\n",
       " 40898: \"lot'\",\n",
       " 52179: 'lookie',\n",
       " 34870: 'fearlessly',\n",
       " 52181: 'libyan',\n",
       " 40899: 'fondles',\n",
       " 35714: 'gopher',\n",
       " 40901: 'wearying',\n",
       " 52182: \"nz's\",\n",
       " 27646: 'minuses',\n",
       " 52183: 'puposelessly',\n",
       " 52184: 'shandling',\n",
       " 31268: 'decapitates',\n",
       " 11929: 'humming',\n",
       " 40902: \"'nother\",\n",
       " 21914: 'smackdown',\n",
       " 30588: 'underdone',\n",
       " 40903: 'frf',\n",
       " 52185: 'triviality',\n",
       " 25248: 'fro',\n",
       " 8777: 'bothers',\n",
       " 52186: \"'kensington\",\n",
       " 73: 'much',\n",
       " 34730: 'muco',\n",
       " 22615: 'wiseguy',\n",
       " 27648: \"richie's\",\n",
       " 40904: 'tonino',\n",
       " 52187: 'unleavened',\n",
       " 11587: 'fry',\n",
       " 40905: \"'tv'\",\n",
       " 40906: 'toning',\n",
       " 14361: 'obese',\n",
       " 30589: 'sensationalized',\n",
       " 40907: 'spiv',\n",
       " 6259: 'spit',\n",
       " 7364: 'arkin',\n",
       " 21915: 'charleton',\n",
       " 16823: 'jeon',\n",
       " 21916: 'boardroom',\n",
       " 4989: 'doubts',\n",
       " 3084: 'spin',\n",
       " 53083: 'hepo',\n",
       " 27649: 'wildcat',\n",
       " 10584: 'venoms',\n",
       " 52191: 'misconstrues',\n",
       " 18514: 'mesmerising',\n",
       " 40908: 'misconstrued',\n",
       " 52192: 'rescinds',\n",
       " 52193: 'prostrate',\n",
       " 40909: 'majid',\n",
       " 16479: 'climbed',\n",
       " 34731: 'canoeing',\n",
       " 52195: 'majin',\n",
       " 57804: 'animie',\n",
       " 40910: 'sylke',\n",
       " 14899: 'conditioned',\n",
       " 40911: 'waddell',\n",
       " 52196: '3\\x85',\n",
       " 41188: 'hyperdrive',\n",
       " 34732: 'conditioner',\n",
       " 53153: 'bricklayer',\n",
       " 2576: 'hong',\n",
       " 52198: 'memoriam',\n",
       " 30592: 'inventively',\n",
       " 25249: \"levant's\",\n",
       " 20638: 'portobello',\n",
       " 52200: 'remand',\n",
       " 19504: 'mummified',\n",
       " 27650: 'honk',\n",
       " 19505: 'spews',\n",
       " 40912: 'visitations',\n",
       " 52201: 'mummifies',\n",
       " 25250: 'cavanaugh',\n",
       " 23385: 'zeon',\n",
       " 40913: \"jungle's\",\n",
       " 34733: 'viertel',\n",
       " 27651: 'frenchmen',\n",
       " 52202: 'torpedoes',\n",
       " 52203: 'schlessinger',\n",
       " 34734: 'torpedoed',\n",
       " 69876: 'blister',\n",
       " 52204: 'cinefest',\n",
       " 34735: 'furlough',\n",
       " 52205: 'mainsequence',\n",
       " 40914: 'mentors',\n",
       " 9094: 'academic',\n",
       " 20602: 'stillness',\n",
       " 40915: 'academia',\n",
       " 52206: 'lonelier',\n",
       " 52207: 'nibby',\n",
       " 52208: \"losers'\",\n",
       " 40916: 'cineastes',\n",
       " 4449: 'corporate',\n",
       " 40917: 'massaging',\n",
       " 30593: 'bellow',\n",
       " 19506: 'absurdities',\n",
       " 53241: 'expetations',\n",
       " 40918: 'nyfiken',\n",
       " 75638: 'mehras',\n",
       " 52209: 'lasse',\n",
       " 52210: 'visability',\n",
       " 33946: 'militarily',\n",
       " 52211: \"elder'\",\n",
       " 19023: 'gainsbourg',\n",
       " 20603: 'hah',\n",
       " 13420: 'hai',\n",
       " 34736: 'haj',\n",
       " 25251: 'hak',\n",
       " 4311: 'hal',\n",
       " 4892: 'ham',\n",
       " 53259: 'duffer',\n",
       " 52213: 'haa',\n",
       " 66: 'had',\n",
       " 11930: 'advancement',\n",
       " 16825: 'hag',\n",
       " 25252: \"hand'\",\n",
       " 13421: 'hay',\n",
       " 20604: 'mcnamara',\n",
       " 52214: \"mozart's\",\n",
       " 30731: 'duffel',\n",
       " 30594: 'haq',\n",
       " 13887: 'har',\n",
       " 44: 'has',\n",
       " 2401: 'hat',\n",
       " 40919: 'hav',\n",
       " 30595: 'haw',\n",
       " 52215: 'figtings',\n",
       " 15495: 'elders',\n",
       " 52216: 'underpanted',\n",
       " 52217: 'pninson',\n",
       " 27652: 'unequivocally',\n",
       " 23673: \"barbara's\",\n",
       " 52219: \"bello'\",\n",
       " 12997: 'indicative',\n",
       " 40920: 'yawnfest',\n",
       " 52220: 'hexploitation',\n",
       " 52221: \"loder's\",\n",
       " 27653: 'sleuthing',\n",
       " 32622: \"justin's\",\n",
       " 52222: \"'ball\",\n",
       " 52223: \"'summer\",\n",
       " 34935: \"'demons'\",\n",
       " 52225: \"mormon's\",\n",
       " 34737: \"laughton's\",\n",
       " 52226: 'debell',\n",
       " 39724: 'shipyard',\n",
       " 30597: 'unabashedly',\n",
       " 40401: 'disks',\n",
       " 2290: 'crowd',\n",
       " 10087: 'crowe',\n",
       " 56434: \"vancouver's\",\n",
       " 34738: 'mosques',\n",
       " 6627: 'crown',\n",
       " 52227: 'culpas',\n",
       " 27654: 'crows',\n",
       " 53344: 'surrell',\n",
       " 52229: 'flowless',\n",
       " 52230: 'sheirk',\n",
       " 40923: \"'three\",\n",
       " 52231: \"peterson'\",\n",
       " 52232: 'ooverall',\n",
       " 40924: 'perchance',\n",
       " 1321: 'bottom',\n",
       " 53363: 'chabert',\n",
       " 52233: 'sneha',\n",
       " 13888: 'inhuman',\n",
       " 52234: 'ichii',\n",
       " 52235: 'ursla',\n",
       " 30598: 'completly',\n",
       " 40925: 'moviedom',\n",
       " 52236: 'raddick',\n",
       " 51995: 'brundage',\n",
       " 40926: 'brigades',\n",
       " 1181: 'starring',\n",
       " 52237: \"'goal'\",\n",
       " 52238: 'caskets',\n",
       " 52239: 'willcock',\n",
       " 52240: \"threesome's\",\n",
       " 52241: \"mosque'\",\n",
       " 52242: \"cover's\",\n",
       " 17637: 'spaceships',\n",
       " 40927: 'anomalous',\n",
       " 27655: 'ptsd',\n",
       " 52243: 'shirdan',\n",
       " 21962: 'obscenity',\n",
       " 30599: 'lemmings',\n",
       " 30600: 'duccio',\n",
       " 52244: \"levene's\",\n",
       " 52245: \"'gorby'\",\n",
       " 25255: \"teenager's\",\n",
       " 5340: 'marshall',\n",
       " 9095: 'honeymoon',\n",
       " 3231: 'shoots',\n",
       " 12258: 'despised',\n",
       " 52246: 'okabasho',\n",
       " 8289: 'fabric',\n",
       " 18515: 'cannavale',\n",
       " 3537: 'raped',\n",
       " 52247: \"tutt's\",\n",
       " 17638: 'grasping',\n",
       " 18516: 'despises',\n",
       " 40928: \"thief's\",\n",
       " 8926: 'rapes',\n",
       " 52248: 'raper',\n",
       " 27656: \"eyre'\",\n",
       " 52249: 'walchek',\n",
       " 23386: \"elmo's\",\n",
       " 40929: 'perfumes',\n",
       " 21918: 'spurting',\n",
       " 52250: \"exposition'\\x85\",\n",
       " 52251: 'denoting',\n",
       " 34740: 'thesaurus',\n",
       " 40930: \"shoot'\",\n",
       " 49759: 'bonejack',\n",
       " 52253: 'simpsonian',\n",
       " 30601: 'hebetude',\n",
       " 34741: \"hallow's\",\n",
       " 52254: 'desperation\\x85',\n",
       " 34742: 'incinerator',\n",
       " 10308: 'congratulations',\n",
       " 52255: 'humbled',\n",
       " 5924: \"else's\",\n",
       " 40845: 'trelkovski',\n",
       " 52256: \"rape'\",\n",
       " 59386: \"'chapters'\",\n",
       " 52257: '1600s',\n",
       " 7253: 'martian',\n",
       " 25256: 'nicest',\n",
       " 52259: 'eyred',\n",
       " 9457: 'passenger',\n",
       " 6041: 'disgrace',\n",
       " 52260: 'moderne',\n",
       " 5120: 'barrymore',\n",
       " 52261: 'yankovich',\n",
       " 40931: 'moderns',\n",
       " 52262: 'studliest',\n",
       " 52263: 'bedsheet',\n",
       " 14900: 'decapitation',\n",
       " 52264: 'slurring',\n",
       " 52265: \"'nunsploitation'\",\n",
       " 34743: \"'character'\",\n",
       " 9880: 'cambodia',\n",
       " 52266: 'rebelious',\n",
       " 27657: 'pasadena',\n",
       " 40932: 'crowne',\n",
       " 52267: \"'bedchamber\",\n",
       " 52268: 'conjectural',\n",
       " 52269: 'appologize',\n",
       " 52270: 'halfassing',\n",
       " 57816: 'paycheque',\n",
       " 20606: 'palms',\n",
       " 52271: \"'islands\",\n",
       " 40933: 'hawked',\n",
       " 21919: 'palme',\n",
       " 40934: 'conservatively',\n",
       " 64007: 'larp',\n",
       " 5558: 'palma',\n",
       " 21920: 'smelling',\n",
       " 12998: 'aragorn',\n",
       " 52272: 'hawker',\n",
       " 52273: 'hawkes',\n",
       " 3975: 'explosions',\n",
       " 8059: 'loren',\n",
       " 52274: \"pyle's\",\n",
       " 6704: 'shootout',\n",
       " 18517: \"mike's\",\n",
       " 52275: \"driscoll's\",\n",
       " 40935: 'cogsworth',\n",
       " 52276: \"britian's\",\n",
       " 34744: 'childs',\n",
       " 52277: \"portrait's\",\n",
       " 3626: 'chain',\n",
       " 2497: 'whoever',\n",
       " 52278: 'puttered',\n",
       " 52279: 'childe',\n",
       " 52280: 'maywether',\n",
       " 3036: 'chair',\n",
       " 52281: \"rance's\",\n",
       " 34745: 'machu',\n",
       " 4517: 'ballet',\n",
       " 34746: 'grapples',\n",
       " 76152: 'summerize',\n",
       " 30603: 'freelance',\n",
       " 52283: \"andrea's\",\n",
       " 52284: '\\x91very',\n",
       " 45879: 'coolidge',\n",
       " 18518: 'mache',\n",
       " 52285: 'balled',\n",
       " 40937: 'grappled',\n",
       " 18519: 'macha',\n",
       " 21921: 'underlining',\n",
       " 5623: 'macho',\n",
       " 19507: 'oversight',\n",
       " 25257: 'machi',\n",
       " 11311: 'verbally',\n",
       " 21922: 'tenacious',\n",
       " 40938: 'windshields',\n",
       " 18557: 'paychecks',\n",
       " 3396: 'jerk',\n",
       " 11931: \"good'\",\n",
       " 34748: 'prancer',\n",
       " 21923: 'prances',\n",
       " 52286: 'olympus',\n",
       " 21924: 'lark',\n",
       " 10785: 'embark',\n",
       " 7365: 'gloomy',\n",
       " 52287: 'jehaan',\n",
       " 52288: 'turaqui',\n",
       " 20607: \"child'\",\n",
       " 2894: 'locked',\n",
       " 52289: 'pranced',\n",
       " 2588: 'exact',\n",
       " 52290: 'unattuned',\n",
       " 783: 'minute',\n",
       " 16118: 'skewed',\n",
       " 40940: 'hodgins',\n",
       " 34749: 'skewer',\n",
       " 52291: 'think\\x85',\n",
       " 38765: 'rosenstein',\n",
       " 52292: 'helmit',\n",
       " 34750: 'wrestlemanias',\n",
       " 16826: 'hindered',\n",
       " 30604: \"martha's\",\n",
       " 52293: 'cheree',\n",
       " 52294: \"pluckin'\",\n",
       " 40941: 'ogles',\n",
       " 11932: 'heavyweight',\n",
       " 82190: 'aada',\n",
       " 11312: 'chopping',\n",
       " 61534: 'strongboy',\n",
       " 41342: 'hegemonic',\n",
       " 40942: 'adorns',\n",
       " 41346: 'xxth',\n",
       " 34751: 'nobuhiro',\n",
       " 52298: 'capitÃ£es',\n",
       " 52299: 'kavogianni',\n",
       " 13422: 'antwerp',\n",
       " 6538: 'celebrated',\n",
       " 52300: 'roarke',\n",
       " 40943: 'baggins',\n",
       " 31270: 'cheeseburgers',\n",
       " 52301: 'matras',\n",
       " 52302: \"nineties'\",\n",
       " 52303: \"'craig'\",\n",
       " 12999: 'celebrates',\n",
       " 3383: 'unintentionally',\n",
       " 14362: 'drafted',\n",
       " 52304: 'climby',\n",
       " 52305: '303',\n",
       " 18520: 'oldies',\n",
       " 9096: 'climbs',\n",
       " 9655: 'honour',\n",
       " 34752: 'plucking',\n",
       " 30074: '305',\n",
       " 5514: 'address',\n",
       " 40944: 'menjou',\n",
       " 42592: \"'freak'\",\n",
       " 19508: 'dwindling',\n",
       " 9458: 'benson',\n",
       " 52307: 'whiteâ€™s',\n",
       " 40945: 'shamelessness',\n",
       " 21925: 'impacted',\n",
       " 52308: 'upatz',\n",
       " 3840: 'cusack',\n",
       " 37567: \"flavia's\",\n",
       " 52309: 'effette',\n",
       " 34753: 'influx',\n",
       " 52310: 'boooooooo',\n",
       " 52311: 'dimitrova',\n",
       " 13423: 'houseman',\n",
       " 25259: 'bigas',\n",
       " 52312: 'boylen',\n",
       " 52313: 'phillipenes',\n",
       " 40946: 'fakery',\n",
       " 27658: \"grandpa's\",\n",
       " 27659: 'darnell',\n",
       " 19509: 'undergone',\n",
       " 52315: 'handbags',\n",
       " 21926: 'perished',\n",
       " 37778: 'pooped',\n",
       " 27660: 'vigour',\n",
       " 3627: 'opposed',\n",
       " 52316: 'etude',\n",
       " 11799: \"caine's\",\n",
       " 52317: 'doozers',\n",
       " 34754: 'photojournals',\n",
       " 52318: 'perishes',\n",
       " 34755: 'constrains',\n",
       " 40948: 'migenes',\n",
       " 30605: 'consoled',\n",
       " 16827: 'alastair',\n",
       " 52319: 'wvs',\n",
       " 52320: 'ooooooh',\n",
       " 34756: 'approving',\n",
       " 40949: 'consoles',\n",
       " 52064: 'disparagement',\n",
       " 52322: 'futureistic',\n",
       " 52323: 'rebounding',\n",
       " 52324: \"'date\",\n",
       " 52325: 'gregoire',\n",
       " 21927: 'rutherford',\n",
       " 34757: 'americanised',\n",
       " 82196: 'novikov',\n",
       " 1042: 'following',\n",
       " 34758: 'munroe',\n",
       " 52326: \"morita'\",\n",
       " 52327: 'christenssen',\n",
       " 23106: 'oatmeal',\n",
       " 25260: 'fossey',\n",
       " 40950: 'livered',\n",
       " 13000: 'listens',\n",
       " 76164: \"'marci\",\n",
       " 52330: \"otis's\",\n",
       " 23387: 'thanking',\n",
       " 16019: 'maude',\n",
       " 34759: 'extensions',\n",
       " 52332: 'ameteurish',\n",
       " 52333: \"commender's\",\n",
       " 27661: 'agricultural',\n",
       " 4518: 'convincingly',\n",
       " 17639: 'fueled',\n",
       " 54014: 'mahattan',\n",
       " 40952: \"paris's\",\n",
       " 52336: 'vulkan',\n",
       " 52337: 'stapes',\n",
       " 52338: 'odysessy',\n",
       " 12259: 'harmon',\n",
       " 4252: 'surfing',\n",
       " 23494: 'halloran',\n",
       " 49580: 'unbelieveably',\n",
       " 52339: \"'offed'\",\n",
       " 30607: 'quadrant',\n",
       " 19510: 'inhabiting',\n",
       " 34760: 'nebbish',\n",
       " 40953: 'forebears',\n",
       " 34761: 'skirmish',\n",
       " 52340: 'ocassionally',\n",
       " 52341: \"'resist\",\n",
       " 21928: 'impactful',\n",
       " 52342: 'spicier',\n",
       " 40954: 'touristy',\n",
       " 52343: \"'football'\",\n",
       " 40955: 'webpage',\n",
       " 52345: 'exurbia',\n",
       " 52346: 'jucier',\n",
       " 14901: 'professors',\n",
       " 34762: 'structuring',\n",
       " 30608: 'jig',\n",
       " 40956: 'overlord',\n",
       " 25261: 'disconnect',\n",
       " 82201: 'sniffle',\n",
       " 40957: 'slimeball',\n",
       " 40958: 'jia',\n",
       " 16828: 'milked',\n",
       " 40959: 'banjoes',\n",
       " 1237: 'jim',\n",
       " 52348: 'workforces',\n",
       " 52349: 'jip',\n",
       " 52350: 'rotweiller',\n",
       " 34763: 'mundaneness',\n",
       " 52351: \"'ninja'\",\n",
       " 11040: \"dead'\",\n",
       " 40960: \"cipriani's\",\n",
       " 20608: 'modestly',\n",
       " 52352: \"professor'\",\n",
       " 40961: 'shacked',\n",
       " 34764: 'bashful',\n",
       " 23388: 'sorter',\n",
       " 16120: 'overpowering',\n",
       " 18521: 'workmanlike',\n",
       " 27662: 'henpecked',\n",
       " 18522: 'sorted',\n",
       " 52354: \"jÅb's\",\n",
       " 52355: \"'always\",\n",
       " 34765: \"'baptists\",\n",
       " 52356: 'dreamcatchers',\n",
       " 52357: \"'silence'\",\n",
       " 21929: 'hickory',\n",
       " 52358: 'fun\\x97yet',\n",
       " 52359: 'breakumentary',\n",
       " 15496: 'didn',\n",
       " 52360: 'didi',\n",
       " 52361: 'pealing',\n",
       " 40962: 'dispite',\n",
       " 25262: \"italy's\",\n",
       " 21930: 'instability',\n",
       " 6539: 'quarter',\n",
       " 12608: 'quartet',\n",
       " 52362: 'padmÃ©',\n",
       " 52363: \"'bleedmedry\",\n",
       " 52364: 'pahalniuk',\n",
       " 52365: 'honduras',\n",
       " 10786: 'bursting',\n",
       " 41465: \"pablo's\",\n",
       " 52367: 'irremediably',\n",
       " 40963: 'presages',\n",
       " 57832: 'bowlegged',\n",
       " 65183: 'dalip',\n",
       " 6260: 'entering',\n",
       " 76172: 'newsradio',\n",
       " 54150: 'presaged',\n",
       " 27663: \"giallo's\",\n",
       " 40964: 'bouyant',\n",
       " 52368: 'amerterish',\n",
       " 18523: 'rajni',\n",
       " 30610: 'leeves',\n",
       " 34767: 'macauley',\n",
       " 612: 'seriously',\n",
       " 52369: 'sugercoma',\n",
       " 52370: 'grimstead',\n",
       " 52371: \"'fairy'\",\n",
       " 30611: 'zenda',\n",
       " 52372: \"'twins'\",\n",
       " 17640: 'realisation',\n",
       " 27664: 'highsmith',\n",
       " 7817: 'raunchy',\n",
       " 40965: 'incentives',\n",
       " 52374: 'flatson',\n",
       " 35097: 'snooker',\n",
       " 16829: 'crazies',\n",
       " 14902: 'crazier',\n",
       " 7094: 'grandma',\n",
       " 52375: 'napunsaktha',\n",
       " 30612: 'workmanship',\n",
       " 52376: 'reisner',\n",
       " 61306: \"sanford's\",\n",
       " 52377: '\\x91doÃ±a',\n",
       " 6108: 'modest',\n",
       " 19153: \"everything's\",\n",
       " 40966: 'hamer',\n",
       " 52379: \"couldn't'\",\n",
       " 13001: 'quibble',\n",
       " 52380: 'socking',\n",
       " 21931: 'tingler',\n",
       " 52381: 'gutman',\n",
       " 40967: 'lachlan',\n",
       " 52382: 'tableaus',\n",
       " 52383: 'headbanger',\n",
       " 2847: 'spoken',\n",
       " 34768: 'cerebrally',\n",
       " 23490: \"'road\",\n",
       " 21932: 'tableaux',\n",
       " 40968: \"proust's\",\n",
       " 40969: 'periodical',\n",
       " 52385: \"shoveller's\",\n",
       " 25263: 'tamara',\n",
       " 17641: 'affords',\n",
       " 3249: 'concert',\n",
       " 87955: \"yara's\",\n",
       " 52386: 'someome',\n",
       " 8424: 'lingering',\n",
       " 41511: \"abraham's\",\n",
       " 34769: 'beesley',\n",
       " 34770: 'cherbourg',\n",
       " 28624: 'kagan',\n",
       " 9097: 'snatch',\n",
       " 9260: \"miyazaki's\",\n",
       " 25264: 'absorbs',\n",
       " 40970: \"koltai's\",\n",
       " 64027: 'tingled',\n",
       " 19511: 'crossroads',\n",
       " 16121: 'rehab',\n",
       " 52389: 'falworth',\n",
       " 52390: 'sequals',\n",
       " ...}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get word index from the IMDB dataset\n",
    "word_index = imdb.get_word_index() # Get the word index mapping from the IMDB dataset\n",
    "\n",
    "# verify the word index maping\n",
    "word_index # Print the word index mapping\n",
    "# The word index is a dictionary mapping words to their integer indices in the dataset.\n",
    "\n",
    "# reverse the word index mapping to get a mapping from indices to words\n",
    "reversce_word_index = dict([(value, key) for (key, value) in word_index.items()]) \n",
    "# Reverse the word index mapping using list comprehension then converts it to a dictionary.\n",
    "# The reversed mapping allows us to convert integer indices back to words.\n",
    "# Print the first 10 entries of the reversed word index mapping\n",
    "for i in range(10):\n",
    "    if i in reversce_word_index:  # Check if the key exists in the dictionary\n",
    "        print(i, \":\", reversce_word_index[i]) # Print the first 10 entries of the reversed mapping\n",
    "# The output shows the first 10 entries of the reversed word index mapping, where each entry is a tuple of (index, word).\n",
    "\n",
    "reversce_word_index\n",
    "\n",
    "# #  reverse the word index mapping to get a mapping from indices to words\n",
    "# reversce_word_index = {value:key for (key, value) in word_index.items()}\n",
    "# # ussing dist comprehension to reverse the word index mapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9474533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncode explaination:\\n- The list comprehension iterates over each index in the sample review.\\n\\n[reversce_word_index.get(i-3, '?') for i in sample_review]:\\n\\nfor i in sample_review\\n    This iterates over each value i in sample_review (which likely contains tokenized word indices from a text dataset).\\n\\nreversce_word_index.get(i-3, '?')\\n    It retrieves the corresponding word from reverse_word_index using .get(i-3, '?').\\n        - i-3: Adjusts the index (sometimes needed in datasets like IMDB reviews where indices 0, 1, 2 are reserved for padding, start-of-sequence, and unknown tokens).\\n        - '?': This is the default value returned if the index i-3 is not found in reverse_word_index.\\n        - .get(i-3, '?'): If i-3 is missing in reverse_word_index, it returns '?', preventing errors.\\n\\n' '.join([...])\\n    ' '.join([...]) merges the words in the list into a single string, with spaces between them, forming a human-readable sentence.\\n\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display first review in the training set\n",
    "# decode the review using the reversed word index mapping\n",
    "sample_review = x_train[1] # Get the first review from the training set\n",
    "decode_review = ' '.join([reversce_word_index.get(i-3, '?') for i in sample_review]) # Decode the review using the reversed word index mapping\n",
    "decode_review\n",
    "\n",
    "'''\n",
    "code explaination:\n",
    "- The list comprehension iterates over each index in the sample review.\n",
    "\n",
    "[reversce_word_index.get(i-3, '?') for i in sample_review]:\n",
    "\n",
    "for i in sample_review\n",
    "    This iterates over each value i in sample_review (which likely contains tokenized word indices from a text dataset).\n",
    "\n",
    "reversce_word_index.get(i-3, '?')\n",
    "    It retrieves the corresponding word from reverse_word_index using .get(i-3, '?').\n",
    "        - i-3: Adjusts the index (sometimes needed in datasets like IMDB reviews where indices 0, 1, 2 are reserved for padding, start-of-sequence, and unknown tokens).\n",
    "        - '?': This is the default value returned if the index i-3 is not found in reverse_word_index.\n",
    "        - .get(i-3, '?'): If i-3 is missing in reverse_word_index, it returns '?', preventing errors.\n",
    "\n",
    "' '.join([...])\n",
    "    ' '.join([...]) merges the words in the list into a single string, with spaces between them, forming a human-readable sentence.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534cbb3b",
   "metadata": {},
   "source": [
    "## <font color = 'yellow'> Pre-Processing using Tensorflow </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc5a07d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 600)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNote:\\n1. sequence.pad_sequences() is used to ensure all sequences have the same length, which is required for neural network models.\\n\\n2. in the above - max_len = 500\\n    This sets the maximum sequence length to 500.\\n    Any sequence shorter than 500 will be padded (usually with zeros).\\n    Any sequence longer than 500 will be truncated.\\n\\n3. after padding the sequence the data\\n    It ensures that all sequences in these datasets are exactly 500 tokens long\\n    - If a sequence is shorter than max_len (500) â†’ Adds padding (default is zeros) at the beginning or end.\\n    - If a sequence is longer than max_len (500) â†’ Truncates extra tokens.\\n    - This ensures consistent input size, which is essential for training deep learning models.\\n\\n4. now contains uniform-length sequences, ready to be passed into a neural network.\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary module for sequence preprocessing in TensorFlow\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Set the maximum sequence length to ensure uniform input size\n",
    "max_len = 600\n",
    "\n",
    "# Apply padding to training data\n",
    "# - If a sequence is shorter than max_len, it is padded with zeros.\n",
    "# - If a sequence is longer than max_len, it is truncated.\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len, padding='pre')\n",
    "\n",
    "# Apply padding to test data using the same approach\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len, padding='pre')\n",
    "\n",
    "# Display the shape of the transformed training data\n",
    "print(x_train.shape)  # Expected output: (num_samples, 600)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Note:\n",
    "1. sequence.pad_sequences() is used to ensure all sequences have the same length, which is required for neural network models.\n",
    "\n",
    "2. in the above - max_len = 500\n",
    "    This sets the maximum sequence length to 500.\n",
    "    Any sequence shorter than 500 will be padded (usually with zeros).\n",
    "    Any sequence longer than 500 will be truncated.\n",
    "\n",
    "3. after padding the sequence the data\n",
    "    It ensures that all sequences in these datasets are exactly 500 tokens long\n",
    "    - If a sequence is shorter than max_len (500) â†’ Adds padding (default is zeros) at the beginning or end.\n",
    "    - If a sequence is longer than max_len (500) â†’ Truncates extra tokens.\n",
    "    - This ensures consistent input size, which is essential for training deep learning models.\n",
    "\n",
    "4. now contains uniform-length sequences, ready to be passed into a neural network.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6dfc29f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   19,  178,   32],\n",
       "       [   0,    0,    0, ...,   16,  145,   95],\n",
       "       [   0,    0,    0, ...,    7,  129,  113],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    4, 3586,    2],\n",
       "       [   0,    0,    0, ...,   12,    9,   23],\n",
       "       [   0,    0,    0, ...,  204,  131,    9]], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e7cd94",
   "metadata": {},
   "source": [
    "## <font color = 'yellow'>Create / Build Recurrent neural network (RNN) Model </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30481dc",
   "metadata": {},
   "source": [
    "### Import Required Libiraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce59022e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n",
      "3.9.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "\n",
    "# Checking the tensorflow libraries\n",
    "print(tensorflow.__version__)\n",
    "\n",
    "# checking the keras libraries\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1f4edef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 600)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aefa22e",
   "metadata": {},
   "source": [
    "### Create Model - RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13a7e773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mohan Govindasamy\\Documents\\AI & ML Course Training\\GitHub_Learnings\\Generative_AI_Learning\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)        â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nExplanation of Components used in Sequential components\\n    - Embedding Layer: Converts words into numerical vectors (word embeddings) for better contextual understanding.\\n    - SimpleRNN Layer: Processes sequences step by step, learning dependencies between words.\\n    - Dense Output Layer: Uses sigmoid activation to output probabilities (ideal for binary classification).\\n    - Model Summary: Displays a summary of layers and parameter counts.\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 01: Initialize Sequential Model\n",
    "# Sequential means layers are added one after another in a sequence.\n",
    "model = keras.Sequential([\n",
    "    \n",
    "    # Step 02: Input Layer - Embedding Layer\n",
    "    # Converts word indices into dense vectors of fixed size (word embeddings).\n",
    "    # max_features: Total number of unique words considered (vocabulary size).\n",
    "    # 256: Each word will be represented as a 256-dimensional vector.\n",
    "    # input_length = max_len: Ensures all sequences are of the same length.\n",
    "    keras.layers.Embedding(input_dim=max_features, output_dim=256, input_length=max_len),\n",
    "    \n",
    "    # Step 03: Hidden Layer - SimpleRNN Layer\n",
    "    # A Simple Recurrent Neural Network (RNN) layer with 256 units.\n",
    "    # It helps process sequences (e.g., sentences) by maintaining memory across inputs.\n",
    "    # Activation function 'relu': Helps introduce non-linearity for better learning.\n",
    "    \n",
    "    # # First RNN layer (returns sequences to pass to the next layer)\n",
    "    # keras.layers.SimpleRNN(256, activation='relu', return_sequences=True),\n",
    "\n",
    "    # Second RNN layer (also returns sequences)\n",
    "    keras.layers.SimpleRNN(128, activation='relu', return_sequences=True),\n",
    "\n",
    "    # Third RNN layer (does NOT return sequences, outputs final state)\n",
    "    keras.layers.SimpleRNN(64, activation='relu'),\n",
    "    \n",
    "    # Step 04: Output Layer - Dense Layer\n",
    "    # A fully connected layer that outputs a single value (binary classification).\n",
    "    # Activation function 'sigmoid': Compresses the output to a probability range (0 to 1).\n",
    "    # Output Layer (binary classification)\n",
    "    keras.layers.Dense(units=1, activation='sigmoid')  # Outputs a single neuron for binary classification.\n",
    "    \n",
    "])\n",
    "\n",
    "# Step 05: Model Summary\n",
    "# Displays the architecture, layer details, and the number of trainable parameters.\n",
    "model.summary()\n",
    "\n",
    "'''\n",
    "Explanation of Components used in Sequential components\n",
    "    - Embedding Layer: Converts words into numerical vectors (word embeddings) for better contextual understanding.\n",
    "    - SimpleRNN Layer: Processes sequences step by step, learning dependencies between words.\n",
    "    - Dense Output Layer: Uses sigmoid activation to output probabilities (ideal for binary classification).\n",
    "    - Model Summary: Displays a summary of layers and parameter counts.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d6c8d",
   "metadata": {},
   "source": [
    "### Compile this Model / neural network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ae3552b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)        â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compile model with\n",
    "# 1. Optimizer: Adam (adaptive learning rate for efficient training)\n",
    "# 2. Loss Function: Suitable for binary classification (0 or 1 output)\n",
    "# 3. Metric: Measures how often predictions match the actual labels\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',               # Optimizer: Adam (adaptive learning rate for efficient training)\n",
    "    loss='binary_crossentropy',     # Loss Function: Suitable for binary classification (0 or 1 output)\n",
    "    metrics=['accuracy']            # Metric: Measures how often predictions match the actual labels\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df1c229",
   "metadata": {},
   "source": [
    "### Setup tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0dc93917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "# creating logs directory\n",
    "log_fs = \"logs/fit/\"+ datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "tensorflow_callback = TensorBoard(log_dir=log_fs, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eea034",
   "metadata": {},
   "source": [
    "### Setup early stoping\n",
    "to prevent overfitting by stopping training once the model's performance on a validation set starts to decline. It helps ensure that the model generalizes well to unseen data rather than memorizing the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af450ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import EarlyStopping callback from Keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define EarlyStopping Callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',           # Monitors validation loss during training\n",
    "    patience=5,                   # Stops training if loss does not improve for 5 consecutive epochs\n",
    "    restore_best_weights=True,    # Restores model weights from the epoch with the best validation loss\n",
    "    mode='min'                    # Ensures the monitored metric is minimized (ideal for loss functions)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d06fcf",
   "metadata": {},
   "source": [
    "### Train Model with Early stoping (along with Test data validations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6decd15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 349ms/step - accuracy: 0.6456 - loss: 112.5745 - val_accuracy: 0.7842 - val_loss: 0.4607\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 419ms/step - accuracy: 0.8283 - loss: 0.3839 - val_accuracy: 0.7474 - val_loss: 0.5016\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 357ms/step - accuracy: 0.8886 - loss: 0.2800 - val_accuracy: 0.8440 - val_loss: 0.3760\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 364ms/step - accuracy: 0.9354 - loss: 0.1715 - val_accuracy: 0.8312 - val_loss: 0.4279\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 366ms/step - accuracy: 0.9555 - loss: 0.1235 - val_accuracy: 0.8391 - val_loss: 0.4764\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 351ms/step - accuracy: 0.9475 - loss: 0.1325 - val_accuracy: 0.8359 - val_loss: 0.5633\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 351ms/step - accuracy: 0.9741 - loss: 0.0782 - val_accuracy: 0.8246 - val_loss: 0.6020\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 360ms/step - accuracy: 0.9532 - loss: 0.1258 - val_accuracy: 0.8225 - val_loss: 0.7191\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚     \u001b[38;5;34m2,560,000\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚        \u001b[38;5;34m49,280\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚        \u001b[38;5;34m12,352\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m65\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,865,093</span> (30.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,865,093\u001b[0m (30.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,621,697</span> (10.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,621,697\u001b[0m (10.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,243,396</span> (20.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m5,243,396\u001b[0m (20.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['accuracy', 'loss', 'val_accuracy', 'val_loss'])\n",
      "Training Accuracy: [0.7020000219345093, 0.8337600231170654, 0.8960400223731995, 0.9301999807357788, 0.949720025062561, 0.9530799984931946, 0.9733200073242188, 0.9650800228118896]\n",
      "Validation Accuracy: [0.7842400074005127, 0.7473599910736084, 0.8440399765968323, 0.8311600089073181, 0.8390799760818481, 0.835919976234436, 0.8245999813079834, 0.8225200176239014]\n",
      "Training Loss: [206.8404998779297, 0.3775540292263031, 0.2615761458873749, 0.1809711903333664, 0.13547435402870178, 0.1238936111330986, 0.07669655233621597, 0.09717259556055069]\n",
      "Validation Loss: [0.4606955349445343, 0.5015568733215332, 0.3760053813457489, 0.427922785282135, 0.4763559103012085, 0.5633460879325867, 0.6020415425300598, 0.7190569043159485]\n"
     ]
    }
   ],
   "source": [
    "# Train the model using training and test data,\n",
    "# while utilizing TensorBoard and EarlyStopping callbacks.\n",
    "history = model.fit(\n",
    "    x_train,               # Training input features\n",
    "    y_train,               # Training labels\n",
    "    validation_data=(x_test, y_test),  # Validation dataset (used to monitor performance) # More precise control over validation set.\n",
    "    # validation_split=0.2,  # Automatically splits 20% of X_train for validation.  #Does not require separate validation data (X_test, y_test).Useful when validation data is not explicitly provided.\n",
    "    batch_size=32,         # Number of samples per gradient update\n",
    "    epochs=10,            # Maximum number of training iterations (epochs) (Note : Shorter training may prevent overfitting but might not be enough for complex tasks.)\n",
    "    callbacks=[tensorflow_callback, early_stopping],  # Callback functions: TensorBoard for visualization & EarlyStopping for optimized training\n",
    "    verbose=1              # Print detailed training progress output\n",
    ")\n",
    "\n",
    "# Display the model architecture, layer details, and parameter count\n",
    "model.summary()\n",
    "\n",
    "# Print the keys available in the training history dictionary\n",
    "# This shows what metrics were tracked during training (e.g., accuracy, loss)\n",
    "print(history.history.keys())\n",
    "\n",
    "# Access training and validation accuracy\n",
    "print(\"Training Accuracy:\", history.history['accuracy'])    # Accuracy on training data\n",
    "print(\"Validation Accuracy:\", history.history['val_accuracy'])  # Accuracy on validation data\n",
    "\n",
    "# Access training and validation loss\n",
    "print(\"Training Loss:\", history.history['loss'])    # Modelâ€™s loss on training data\n",
    "print(\"Validation Loss:\", history.history['val_loss'])  # Modelâ€™s loss on validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac25b9b",
   "metadata": {},
   "source": [
    "### Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9236a4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# save HDF5 model\n",
    "model.save('my_rnn_model.h5')\n",
    "\n",
    "# save Keras model\n",
    "model.save('my_rnn_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d56d598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac59c9a",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "559c92b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# load extention for tensorboard\n",
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "480bec6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 135352), started 0:00:21 ago. (Use '!kill 135352' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-acf9ded7fc75735a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-acf9ded7fc75735a\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fb2fde",
   "metadata": {},
   "source": [
    "### Extra for automated save model\n",
    "Automate model saving during training using ModelCheckpoint, a Keras callback that saves the model at different stages of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b1a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Define model parameters\n",
    "max_features = 10000  # Vocabulary size\n",
    "max_len = 500         # Maximum sequence length\n",
    "\n",
    "# Step 1: Initialize Model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_features, output_dim=256, input_length=max_len),  # Word Embeddings\n",
    "    SimpleRNN(256, activation='relu', return_sequences=True),  # First RNN Layer\n",
    "    SimpleRNN(128, activation='relu', return_sequences=True),  # Second RNN Layer\n",
    "    SimpleRNN(64, activation='relu'),  # Third RNN Layer (Final hidden layer)\n",
    "    Dense(1, activation='sigmoid')  # Output Layer (Binary Classification)\n",
    "])\n",
    "\n",
    "# Step 2: Compile Model\n",
    "model.compile(\n",
    "    optimizer='adam',               # Adaptive learning rate optimizer\n",
    "    loss='binary_crossentropy',      # Suitable for binary classification tasks\n",
    "    metrics=['accuracy']             # Track accuracy during training\n",
    ")\n",
    "\n",
    "# Step 3: Define Callbacks for Automatic Model Saving & Early Stopping\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='best_model.keras',      # Save best model file\n",
    "    monitor='val_loss',               # Monitor validation loss\n",
    "    save_best_only=True,              # Save only when improvement is detected\n",
    "    save_weights_only=False,          # Save the full model\n",
    "    mode='min',                        # Minimize validation loss\n",
    "    verbose=1                          # Show save status\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',                # Stop training if validation loss stagnates\n",
    "    patience=5,                         # Allow 5 epochs of no improvement before stopping\n",
    "    restore_best_weights=True,         # Load best weights after stopping\n",
    "    mode='min',                         # Minimize validation loss\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Step 4: Train Model with Training Data\n",
    "history = model.fit(\n",
    "    x_train, y_train,                 # Training data\n",
    "    validation_data=(x_test, y_test),  # Validation data\n",
    "    epochs=50,                         # Maximum epochs\n",
    "    batch_size=32,                     # Batch size for efficiency\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],  # Apply callbacks\n",
    "    verbose=1                          # Print training progress\n",
    ")\n",
    "\n",
    "# Step 5: Save Model Manually After Training\n",
    "model.save('final_model.keras')  # Save final trained model\n",
    "\n",
    "# Step 6: Load the Best Model After Training\n",
    "best_model = load_model('best_model.keras')\n",
    "\n",
    "# Step 7: Print Available Keys in Training History\n",
    "print(\"Available Metrics:\", history.history.keys())\n",
    "\n",
    "# Step 8: Access Training and Validation Performance\n",
    "print(\"Training Accuracy:\", history.history['accuracy'])\n",
    "print(\"Validation Accuracy:\", history.history['val_accuracy'])\n",
    "print(\"Training Loss:\", history.history['loss'])\n",
    "print(\"Validation Loss:\", history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b57e13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
